import logging

import aiohttp
import requests
import asyncio
from typing import Dict, List, Optional, Set, Tuple
from datetime import datetime
import re
from urllib.parse import quote

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.DEBUG  # –ò–∑–º–µ–Ω—è–µ–º —É—Ä–æ–≤–µ–Ω—å –Ω–∞ DEBUG
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è API
WIKIDATA_API_URL = "https://www.wikidata.org/w/api.php"
WIKIDATA_SPARQL_URL = "https://query.wikidata.org/sparql"
WIKIPEDIA_API_URL = "https://ru.wikipedia.org/w/api.php"

# –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
cache = {
    'city_ids': {},
    'page_info': {},
    'related_articles': {},
    'search_results': {}
}

# –í—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —ç–ø–æ—Ö
ERA_RANGES = {
    'ancient_rus': {'start': '0800', 'end': '1547', 'name': '–î—Ä–µ–≤–Ω—è—è –†—É—Å—å (IX-XVI –≤–≤.)'},
    'tsar_rus': {'start': '1547', 'end': '1721', 'name': '–¶–∞—Ä—Å–∫–∞—è –†–æ—Å—Å–∏—è (XVI-XVIII –≤–≤.)'},
    'imperial': {'start': '1721', 'end': '1917', 'name': '–ò–º–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∞—è –†–æ—Å—Å–∏—è (XVIII-XX –≤–≤.)'},
    'soviet': {'start': '1917', 'end': '1991', 'name': '–°–æ–≤–µ—Ç—Å–∫–∏–π –ø–µ—Ä–∏–æ–¥ (1917-1991)'},
    'modern': {'start': '1991', 'end': str(datetime.now().year), 'name': '–ù–∞—à–µ –≤—Ä–µ–º—è (—Å 1991)'}
}

async def get_city_wikidata_id(city_name: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç ID –≥–æ—Ä–æ–¥–∞ –≤ Wikidata —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞."""
    if city_name in cache['city_ids']:
        return cache['city_ids'][city_name]

    try:
        params = {
            'action': 'wbsearchentities',
            'format': 'json',
            'language': 'ru',
            'type': 'item',
            'search': city_name
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(WIKIDATA_API_URL, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('search'):
                        city_id = data['search'][0]['id']
                        cache['city_ids'][city_name] = city_id
                        return city_id
        return None
    except Exception as e:
        logger.error(f"Error getting Wikidata ID for city {city_name}: {e}")
        return None

async def fetch_wikidata_events(city_id: str, era: str, exclude_events: Set[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ Wikidata —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º."""
    try:
        era_range = ERA_RANGES.get(era)
        if not era_range:
            logger.error(f"Invalid era: {era}")
            return []

        logger.info(f"Fetching Wikidata events for city_id: {city_id}, era: {era}")
        logger.info(f"Era range: {era_range['start']} - {era_range['end']}")

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≥–æ—Ä–æ–¥–∞
        city_coords = await get_city_coordinates(city_id)
        if city_coords:
            logger.info(f"Using city coordinates as fallback: {city_coords}")

        # –ò–∑–º–µ–Ω–µ–Ω–Ω—ã–π SPARQL –∑–∞–ø—Ä–æ—Å —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º –ø–æ–∏—Å–∫–æ–º –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
        query = f"""
        SELECT DISTINCT ?event ?eventLabel ?date ?description ?coord WHERE {{
          ?event wdt:P31 wd:Q1190554;  # instance of historical event
                wdt:P585 ?date.        # point in time
          
          # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ª–æ–∫–∞—Ü–∏–∏
          {{
            ?event wdt:P276 ?city.     # direct location
            ?city wdt:P31 wd:Q515;     # instance of city
            wd:{city_id} ?city.        # city ID
          }} UNION {{
            ?event wdt:P276 ?location. # location
            ?location wdt:P131* wd:{city_id}. # administrative unit of city
          }}
          
          OPTIONAL {{ ?event schema:description ?description FILTER(LANG(?description) = "ru") }}
          OPTIONAL {{ ?event wdt:P625 ?coord }}  # coordinates
          FILTER(?date >= "{era_range['start']}-01-01T00:00:00Z"^^xsd:dateTime &&
                 ?date <= "{era_range['end']}-12-31T23:59:59Z"^^xsd:dateTime)
          SERVICE wikibase:label {{ bd:serviceParam wikibase:language "[AUTO_LANGUAGE],ru". }}
        }}
        ORDER BY ?date
        LIMIT 100
        """

        logger.debug(f"Wikidata SPARQL query: {query}")

        headers = {
            'Accept': 'application/sparql-results+json',
            'User-Agent': 'HistoricalEventsBot/1.0'
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(WIKIDATA_SPARQL_URL, params={'query': query}, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.debug(f"Wikidata raw response: {data}")
                    
                    events = []
                    for result in data.get('results', {}).get('bindings', []):
                        event = {
                            'label': result.get('eventLabel', {}).get('value', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ'),
                            'date': result.get('date', {}).get('value', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –¥–∞—Ç–∞'),
                            'description': result.get('description', {}).get('value', '')
                        }
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        if 'coord' in result:
                            coord_value = result['coord']['value']
                            try:
                                # –ü–∞—Ä—Å–∏–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ Point(lon lat)
                                match = re.match(r'Point\(([-\d.]+) ([-\d.]+)\)', coord_value)
                                if match:
                                    lon, lat = map(float, match.groups())
                                    event['coordinates'] = [lat, lon]
                                    logger.debug(f"Added event coordinates for {event['label']}: {lat}, {lon}")
                            except Exception as e:
                                logger.error(f"Error parsing coordinates for event {event['label']}: {e}")
                                if city_coords:
                                    event['coordinates'] = city_coords
                                    logger.info(f"Using city coordinates for event {event['label']} after parsing error")
                        elif city_coords:
                            # –ï—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–æ–±—ã—Ç–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≥–æ—Ä–æ–¥–∞
                            event['coordinates'] = city_coords
                            logger.info(f"Using city coordinates for event {event['label']} (no event coordinates)")
                        
                        if exclude_events is None or event['label'] not in exclude_events:
                            events.append(event)
                            logger.debug(f"Added Wikidata event: {event['label']} ({event['date']})")
                    
                    logger.info(f"Successfully fetched {len(events)} events from Wikidata")
                    if not events:
                        logger.warning("No events found in Wikidata response")
                    return events
                else:
                    response_text = await response.text()
                    logger.error(f"Wikidata SPARQL request failed with status {response.status}")
                    logger.error(f"Response: {response_text}")
                    return []
    except Exception as e:
        logger.error(f"Error fetching Wikidata events: {e}", exc_info=True)
        return []

async def fetch_wikipedia_events(city: str, era: str, exclude_events: Set[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ Wikipedia."""
    try:
        era_range = ERA_RANGES.get(era)
        if not era_range:
            logger.error(f"Invalid era: {era}")
            return []

        logger.info(f"Fetching Wikipedia events for city: {city}, era: {era}")

        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_queries = [
            f"{city} –∏—Å—Ç–æ—Ä–∏—è {era_range['start']}-{era_range['end']}",
            f"{city} —Å–æ–±—ã—Ç–∏—è {era_range['start']}-{era_range['end']}",
            f"{city} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è",
            f"{city} {era_range['name']}"
        ]

        logger.info(f"Search queries: {search_queries}")

        found_pages = set()
        events = []

        for query in search_queries:
            if query in cache['search_results']:
                logger.info(f"Using cached results for query: {query}")
                found_pages.update(cache['search_results'][query])
                continue

            try:
                params = {
                    'action': 'query',
                    'format': 'json',
                    'list': 'search',
                    'srsearch': query,
                    'srlimit': 50,
                    'srprop': 'snippet|title',
                    'srnamespace': 0
                }

                logger.debug(f"Wikipedia search params: {params}")

                async with aiohttp.ClientSession() as session:
                    async with session.get(WIKIPEDIA_API_URL, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            if 'query' in data and 'search' in data['query']:
                                page_ids = [str(page['pageid']) for page in data['query']['search']]
                                found_pages.update(page_ids)
                                cache['search_results'][query] = page_ids
                                logger.info(f"Found {len(page_ids)} pages for query: {query}")
                            else:
                                logger.warning(f"No search results for query: {query}")
                                logger.debug(f"Wikipedia response: {data}")
                            await asyncio.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            except Exception as e:
                logger.error(f"Error in Wikipedia search for query '{query}': {e}", exc_info=True)
                continue

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        if found_pages:
            logger.info(f"Processing {len(found_pages)} found pages")
            events.extend(await fetch_pages_info(list(found_pages), city, era_range, exclude_events))
            logger.info(f"Extracted {len(events)} events from Wikipedia pages")
        else:
            logger.warning("No pages found in Wikipedia search")

        return events
    except Exception as e:
        logger.error(f"Error in fetch_wikipedia_events: {e}", exc_info=True)
        return []

async def fetch_pages_info(page_ids: List[str], city: str, era_range: Dict, exclude_events: Set[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö Wikipedia."""
    events = []
    processed_page_ids = set()
    processed_titles = set()

    logger.info(f"Fetching info for {len(page_ids)} Wikipedia pages")

    # –†–∞–∑–±–∏–≤–∞–µ–º page_ids –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ 35 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö URL
    for i in range(0, len(page_ids), 35):
        group = page_ids[i:i + 35]
        group_key = ','.join(group)
        
        logger.info(f"Processing group {i//35 + 1} of {(len(page_ids) + 34)//35} ({len(group)} pages)")
        
        if group_key in cache['page_info']:
            logger.info(f"Using cached info for group {i//35 + 1}")
            events.extend(await process_cached_pages(cache['page_info'][group_key], city, era_range, exclude_events, processed_page_ids, processed_titles))
            continue

        try:
            params = {
                'action': 'query',
                'format': 'json',
                'pageids': '|'.join(group),
                'prop': 'extracts|pageimages|info|categories',
                'exintro': 1,
                'explaintext': 1,
                'inprop': 'url',
                'cllimit': 50
            }

            logger.debug(f"Wikipedia API params for group {i//35 + 1}: {params}")

            async with aiohttp.ClientSession() as session:
                async with session.get(WIKIPEDIA_API_URL, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'query' in data and 'pages' in data['query']:
                            pages = data['query']['pages']
                            logger.info(f"Successfully retrieved info for {len(pages)} pages in group {i//35 + 1}")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
                            for page_id, page in pages.items():
                                if 'missing' in page:
                                    logger.warning(f"Page {page_id} is missing: {page.get('title', 'Unknown')}")
                                if 'invalid' in page:
                                    logger.warning(f"Page {page_id} is invalid: {page.get('title', 'Unknown')}")
                            
                            cache['page_info'][group_key] = pages
                            events.extend(await process_pages(pages, city, era_range, exclude_events, processed_page_ids, processed_titles))
                        else:
                            logger.error(f"Invalid response format for group {i//35 + 1}")
                            logger.debug(f"Response data: {data}")
                    else:
                        response_text = await response.text()
                        logger.error(f"Wikipedia API request failed for group {i//35 + 1} with status {response.status}")
                        logger.error(f"Response: {response_text}")
                    await asyncio.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        except Exception as e:
            logger.error(f"Error fetching page info for group {i//35 + 1}: {e}", exc_info=True)
            continue

    logger.info(f"Total events found after processing all pages: {len(events)}")
    return events

async def process_pages(pages: Dict, city: str, era_range: Dict, exclude_events: Set[str], 
                       processed_page_ids: Set[str], processed_titles: Set[str]) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã Wikipedia –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è."""
    events = []
    
    logger.info(f"Processing {len(pages)} Wikipedia pages for city: {city}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≥–æ—Ä–æ–¥–∞
    city_id = await get_city_wikidata_id(city)
    city_coords = None
    if city_id:
        city_coords = await get_city_coordinates(city_id)
        if city_coords:
            logger.info(f"Using city coordinates for events: {city_coords}")
    
    # –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ù–ï —è–≤–ª—è–µ—Ç—Å—è —Å–æ–±—ã—Ç–∏–µ–º
    non_event_keywords = [
        '—Å–ø–∏—Å–æ–∫', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '—à–∞–±–ª–æ–Ω', '–ø—Ä–æ–µ–∫—Ç', '–ø–æ—Ä—Ç–∞–ª', '–≤–∏–∫–∏–ø–µ–¥–∏—è',
        '—Ä–æ—Å—Å–∏–π—Å–∫–∞—è –∏–º–ø–µ—Ä–∏—è', '–∏—Å—Ç–æ—Ä–∏—è —Ä–æ—Å—Å–∏–∏', '—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è', '—ç–ø–æ—Ö–∞',
        '–ø–µ—Ä–∏–æ–¥', '–≤–µ–∫', '–≥–æ–¥—ã', '–≥–æ–¥–∞', '–≥–æ–¥—É', '–≥–æ–¥–∞—Ö'
    ]
    
    # –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Å–æ–±—ã—Ç–∏—è
    event_category_keywords = [
        '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è', '—Å–æ–±—ã—Ç–∏—è –ø–æ –≥–æ–¥–∞–º', '—Å–æ–±—ã—Ç–∏—è –ø–æ –º–µ—Å—è—Ü–∞–º',
        '—Å–æ–±—ã—Ç–∏—è –ø–æ –¥–Ω—è–º', '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞—Ç—ã', '–≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è',
        '–∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è', '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã'
    ]
    
    # –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Å–æ–±—ã—Ç–∏–µ
    event_title_keywords = [
        '—Å–æ–±—ã—Ç–∏–µ', '—Å—Ä–∞–∂–µ–Ω–∏–µ', '–±–∏—Ç–≤–∞', '–≤–æ–π–Ω–∞', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–≤–æ—Å—Å—Ç–∞–Ω–∏–µ',
        '–ø–æ–∂–∞—Ä', '–Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–æ—Å–Ω–æ–≤–∞–Ω–∏–µ', '—Å–æ–∑–¥–∞–Ω–∏–µ',
        '–ø–æ—Å—Ç—Ä–æ–µ–Ω', '–ø–æ—Å—Ç—Ä–æ–µ–Ω–∞', '–ø–æ—Å—Ç—Ä–æ–µ–Ω–æ', '–∑–∞–ª–æ–∂–µ–Ω', '–∑–∞–ª–æ–∂–µ–Ω–∞',
        '–∑–∞–ª–æ–∂–µ–Ω–æ', '—É—á—Ä–µ–∂–¥–µ–Ω', '—É—á—Ä–µ–∂–¥–µ–Ω–∞', '—É—á—Ä–µ–∂–¥–µ–Ω–æ'
    ]
    
    for page_id, page in pages.items():
        if page_id in processed_page_ids or page.get('title') in processed_titles:
            logger.debug(f"Skipping already processed page: {page.get('title')}")
            continue

        processed_page_ids.add(page_id)
        processed_titles.add(page.get('title', ''))
        
        title = page.get('title', '').lower()
        text = page.get('extract', '').lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ-—Å–æ–±—ã—Ç–∏–µ–º
        if any(keyword in title.lower() for keyword in non_event_keywords):
            logger.debug(f"Page {title} skipped - matches non-event keywords")
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = [cat.get('title', '').lower() for cat in page.get('categories', [])]
        logger.debug(f"Page {title} categories: {categories}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ–±—ã—Ç–∏–π
        has_event_category = any(
            any(keyword in cat for keyword in event_category_keywords)
            for cat in categories
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–æ–±—ã—Ç–∏–π
        has_event_title = any(keyword in title for keyword in event_title_keywords)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç–µ
        has_dates = bool(find_dates_in_text(text))
        
        # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ö–æ—Ç—è –±—ã –¥–≤—É–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏–∑ —Ç—Ä–µ—Ö
        if sum([has_event_category, has_event_title, has_dates]) < 2:
            logger.debug(f"Page {title} skipped - insufficient event indicators")
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≥–æ—Ä–æ–¥—É
        city_mentions = text.count(city.lower())
        logger.debug(f"Page {title} city mentions: {city_mentions}")
        
        if city_mentions < 1:
            logger.debug(f"Page {title} skipped - insufficient city mentions")
            continue

        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
        dates = find_dates_in_text(text)
        logger.debug(f"Page {title} found dates: {[d.strftime('%Y-%m-%d') for d in dates]}")
        
        # –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≥–æ–¥–æ–≤ –∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–±—ã—Ç–∏—è
        if not dates and has_event_title and any(str(year) in text for year in range(int(era_range['start']), int(era_range['end']) + 1)):
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ —Å –ø—Ä–∏–º–µ—Ä–Ω–æ–π –¥–∞—Ç–æ–π
            event = {
                'label': page.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ'),
                'date': f"{era_range['start']}-01-01",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä–Ω—É—é –¥–∞—Ç—É
                'description': text[:500] + '...' if len(text) > 500 else text,
                'url': page.get('fullurl', '')
            }
            if city_coords:
                event['coordinates'] = city_coords
            if exclude_events is None or event['label'] not in exclude_events:
                events.append(event)
                logger.info(f"Added event with approximate date from page {title}")
            continue
        
        for date in dates:
            if is_date_in_range(date, era_range):
                event = {
                    'label': page.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ'),
                    'date': date.strftime('%Y-%m-%d'),
                    'description': text[:500] + '...' if len(text) > 500 else text,
                    'url': page.get('fullurl', '')
                }
                if city_coords:
                    event['coordinates'] = city_coords
                
                if exclude_events is None or event['label'] not in exclude_events:
                    events.append(event)
                    logger.info(f"Added event from page {title} with date {date.strftime('%Y-%m-%d')}")
            else:
                logger.debug(f"Date {date.strftime('%Y-%m-%d')} from page {title} outside era range")

    logger.info(f"Processed pages resulted in {len(events)} events")
    return events

def find_dates_in_text(text: str) -> List[datetime]:
    """–ò—â–µ—Ç –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ."""
    dates = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—Ç
    patterns = [
        r'\b(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})\b',
        r'\b(\d{4})[-‚Äì](\d{1,2})[-‚Äì](\d{1,2})\b',
        r'\b(\d{1,2})[-‚Äì](\d{1,2})[-‚Äì](\d{4})\b'
    ]
    
    month_map = {
        '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
        '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
        '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
    }
    
    logger.debug(f"Searching for dates in text of length: {len(text)}")
    
    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                if '—è–Ω–≤–∞—Ä—è' in pattern:  # –†—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                    day, month, year = match.groups()
                    month = month_map[month.lower()]
                    logger.debug(f"Found Russian date: {day}.{month}.{year}")
                else:  # ISO —Ñ–æ—Ä–º–∞—Ç
                    if pattern == r'\b(\d{4})[-‚Äì](\d{1,2})[-‚Äì](\d{1,2})\b':
                        year, month, day = map(int, match.groups())
                        logger.debug(f"Found ISO date (YMD): {year}-{month}-{day}")
                    else:
                        day, month, year = map(int, match.groups())
                        logger.debug(f"Found ISO date (DMY): {day}-{month}-{year}")
                
                date = datetime(int(year), int(month), int(day))
                if 800 <= date.year <= 2100:  # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞—Ç—ã
                    dates.append(date)
                    logger.debug(f"Added valid date: {date.strftime('%Y-%m-%d')}")
                else:
                    logger.debug(f"Skipped date outside valid range: {date.strftime('%Y-%m-%d')}")
            except (ValueError, KeyError) as e:
                logger.debug(f"Error parsing date: {match.group()} - {str(e)}")
                continue
    
    logger.info(f"Found {len(dates)} valid dates in text")
    return dates

def is_date_in_range(date: datetime, era_range: Dict) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –¥–∞—Ç–∞ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ."""
    start_year = int(era_range['start'])
    end_year = int(era_range['end'])
    return start_year <= date.year <= end_year

async def get_combined_events(city: str, era: str, exclude_events: Set[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    logger.info(f"Starting combined search for city: {city}, era: {era}")
    
    city_id = await get_city_wikidata_id(city)
    if not city_id:
        logger.error(f"Could not find Wikidata ID for city: {city}")
        return []
    
    logger.info(f"Found Wikidata ID for {city}: {city_id}")

    # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—ã—Ç–∏—è –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    try:
        wikidata_events, wikipedia_events = await asyncio.gather(
            fetch_wikidata_events(city_id, era, exclude_events),
            fetch_wikipedia_events(city, era, exclude_events)
        )
        
        logger.info(f"Found {len(wikidata_events)} events from Wikidata and {len(wikipedia_events)} events from Wikipedia")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É–¥–∞–ª—è—è –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_events = []
        seen_titles = set()

        for event in wikidata_events + wikipedia_events:
            if event['label'] not in seen_titles:
                seen_titles.add(event['label'])
                all_events.append(event)

        logger.info(f"Combined unique events: {len(all_events)}")
        return all_events
    except Exception as e:
        logger.error(f"Error in get_combined_events: {e}", exc_info=True)
        return []

def format_event_message(event: Dict, city: str) -> Tuple[str, str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Å–æ–±—ã—Ç–∏–∏ –¥–ª—è Telegram."""
    try:
        date = datetime.fromisoformat(event['date'].replace('Z', '+00:00'))
        formatted_date = date.strftime('%d.%m.%Y')
    except:
        formatted_date = event['date']

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = f"<b>üìÖ {formatted_date}</b>\n\n"
    message += f"<b>üìú {event['label']}</b>\n"

    if event.get('description'):
        message += f"\nüìù {event['description']}\n"

    message += f"\nüèô {city}\n"

    # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –∫–∞—Ä—Ç—ã
    event_label = quote(event['label'])
    formatted_date_url = quote(formatted_date)
    city_url = quote(city)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
    logger.info(f"Processing event in format_event_message: {event['label']}")
    logger.info(f"Event data: {event}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ URL, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    coords_param = ""
    if 'coordinates' in event:
        try:
            lat, lon = event['coordinates']
            coords_param = f"&lat={lat}&lon={lon}"
            logger.info(f"Adding coordinates to URL for event {event['label']}: lat={lat}, lon={lon}")
        except Exception as e:
            logger.error(f"Error processing coordinates for event {event['label']}: {e}")
            logger.error(f"Coordinates value: {event.get('coordinates')}")
    else:
        logger.info(f"No coordinates found for event {event['label']}")
    
    url = f"https://mnstupichev.github.io/History-project/?event={event_label}&date={formatted_date_url}&city={city_url}{coords_param}"
    logger.info(f"Generated URL for event {event['label']}: {url}")

    return message, url

async def process_cached_pages(pages: Dict, city: str, era_range: Dict, exclude_events: Set[str],
                            processed_page_ids: Set[str], processed_titles: Set[str]) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∫—ç—à–∞."""
    return await process_pages(pages, city, era_range, exclude_events, processed_page_ids, processed_titles)

async def get_city_coordinates(city_id: str) -> Optional[Tuple[float, float]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≥–æ—Ä–æ–¥–∞ –∏–∑ Wikidata."""
    try:
        query = f"""
        SELECT ?lat ?lon WHERE {{
          wd:{city_id} wdt:P625 ?coord.
          BIND(xsd:decimal(strbefore(strafter(str(?coord), "Point("), " ")) AS ?lon)
          BIND(xsd:decimal(strbefore(strafter(str(?coord), " "), ")")) AS ?lat)
        }}
        LIMIT 1
        """

        logger.debug(f"Fetching coordinates for city {city_id}")
        
        headers = {
            'Accept': 'application/sparql-results+json',
            'User-Agent': 'HistoricalEventsBot/1.0'
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(WIKIDATA_SPARQL_URL, params={'query': query}, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    results = data.get('results', {}).get('bindings', [])
                    if results:
                        lat = float(results[0]['lat']['value'])
                        lon = float(results[0]['lon']['value'])
                        logger.info(f"Found coordinates for city {city_id}: {lat}, {lon}")
                        return lat, lon
                    else:
                        logger.warning(f"No coordinates found for city {city_id}")
                        return None
                else:
                    response_text = await response.text()
                    logger.error(f"Failed to get coordinates for city {city_id}: {response.status}")
                    logger.error(f"Response: {response_text}")
                    return None
    except Exception as e:
        logger.error(f"Error getting coordinates for city {city_id}: {e}", exc_info=True)
        return None 